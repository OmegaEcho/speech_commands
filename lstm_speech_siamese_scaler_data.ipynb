{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a notebook for speech siamese. \n",
    "the goal is to add siamese network after the speech command network to make a one-shot speech command model. with this model, take two piece of audio as input, the model will tell if it is the same speech command or not. \n",
    "if the accuracy is good enough, we make take it input product for voice trigger or voice command which are useful for all kind of product. \n",
    "\n",
    "the trick may be if siamese can make one shot accure enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import hashlib\n",
    "import math, time, datetime\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa as rosa\n",
    "import librosa.display\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Lambda, BatchNormalization, Activation, LSTM, GRU\n",
    "#from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "#from tensorflow.python.ops import io_ops\n",
    "#from tensorflow.python.platform import gfile\n",
    "#from tensorflow.python.util import compat\n",
    "\n",
    "\n",
    "# binary data is n * 32 * 128\n",
    "default_mfcc_length = 32\n",
    "default_number_of_mfcc = 128\n",
    "\n",
    "default_epochs=10\n",
    "default_batch_size=32\n",
    "\n",
    "# data have been writed to binary file, no need to input words, just for reference\n",
    "#reference start\n",
    "default_wanted_words=[\"one\", \"two\", \"bed\", \"backward\", \"bird\", \"cat\", \"dog\", \"eight\", \"five\", \"follow\", \"forward\", \"four\", \"go\", \"happy\", \"house\", \"learn\", \"left\", \"marvin\", \"nine\", \"no\", \"off\", \"right\", \"seven\", \"sheila\", \"stop\", \"three\", \"tree\", \"visual\", \"wow\", \"zero\",\"up\"]\n",
    "default_train_words=[\"one\", \"two\", \"cat\", \"dog\", \"bed\", \"backward\", \"eight\",\"forward\", \"four\", \"go\", \"happy\", \"house\", \"learn\", \"left\", \"marvin\", \"nine\", \"no\", \"off\", \"right\", \"seven\", \"sheila\", \"stop\", \"three\", \"tree\", \"visual\", \"wow\", \"zero\",\"up\"]\n",
    "default_test_words=[\"five\", \"follow\", \"bird\"]\n",
    "#reference end\n",
    "\n",
    "mfcc_data_train_mfcc1 = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/train_scaler/mfcc1_train_scaler_(50000, 32, 128)_2018-11-08.npy\"\n",
    "mfcc_data_train_mfcc2 = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/train_scaler/mfcc2_train_scaler_(50000, 32, 128)_2018-11-08.npy\"\n",
    "mfcc_data_train_label = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/train_scaler/label_train_scaler_(50000,)_2018-11-08.npy\"\n",
    "\n",
    "mfcc_data_val_mfcc1 = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/validation_scaler/mfcc1_validation_scaler_(100, 32, 128)_2018-11-08.npy\"\n",
    "mfcc_data_val_mfcc2 = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/validation_scaler/mfcc2_validation_scaler_(100, 32, 128)_2018-11-08.npy\"\n",
    "mfcc_data_val_label = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/validation_scaler/label_validation_scaler_(100,)_2018-11-08.npy\"\n",
    "\n",
    "mfcc_data_test_mfcc1 = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/test_scaler/mfcc1_test_scaler_(100, 32, 128)_2018-11-08.npy\"\n",
    "mfcc_data_test_mfcc2 = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/test_scaler/mfcc2_test_scaler_(100, 32, 128)_2018-11-08.npy\"\n",
    "mfcc_data_test_label = \"/home/zhangjun/tensorflow/speech_siamese_zj/binary_data/test_scaler/same_test_scaler_(100,)_2018-11-08.npy\"\n",
    "\n",
    "\n",
    "default_model_path=\"/home/zhangjun/tensorflow/speech_siamese_zj/trained\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a keras lstm network, take mfcc vector as input.\n",
    "\n",
    "the speech command mfcc input shape is (?, mfcc_number, hop_number, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mfcc_data(file_name):\n",
    "    f = open(file_name, \"rb\")\n",
    "    t = np.load(f)\n",
    "    print(\"shape:\", t.shape)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(local_input_shape, is_training=True):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, return_sequences=False, stateful=False, input_shape=local_input_shape))\n",
    "    #model.add(GRU(256, return_sequences=True, stateful=False))\n",
    "    #model.add(GRU(256, stateful=False))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"sigmoid\")) \n",
    "    #if (is_training):\n",
    "    #    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(labels_count, activation=\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_siamese_model(input_shape, siamese_mode = 'concat'):\n",
    "    right_input = Input(input_shape)\n",
    "    left_input = Input(input_shape)\n",
    "    keras_model = create_lstm_model(input_shape)\n",
    "    \n",
    "    right_encoder = keras_model(right_input)\n",
    "    left_encoder = keras_model(left_input)\n",
    "    if (siamese_mode == 'minus'):\n",
    "        concatenated_layer = Lambda(lambda x: x[0]-x[1], output_shape=lambda x: x[0])([right_encoder, left_encoder])\n",
    "    elif (siamese_mode == 'abs'):\n",
    "        concatenated_layer = Lambda(lambda x: tf.abs(x[0]-x[1]), output_shape=lambda x: x[0])([right_encoder, left_encoder])\n",
    "    #elif (siamese_mode == \"eu\"):\n",
    "    #    concatenated_layer = Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x[0]-x[1]), 2)), output_shape=lambda x: x[0])([right_encoder, left_encoder])\n",
    "    else:\n",
    "        raise ValueError(\"unknown siamese_mode\")\n",
    "        \n",
    "    output_layer = Dense(1, activation='sigmoid')(concatenated_layer)\n",
    "    siamese_model = Model([right_input, left_input], output_layer)\n",
    "    return siamese_model\n",
    "    \n",
    "def siamese_train(local_siamese_mode='abs'):\n",
    "    siamese_model = create_siamese_model((default_mfcc_length, default_number_of_mfcc), siamese_mode=local_siamese_mode)\n",
    "\n",
    "    siamese_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    x1_train = read_mfcc_data(mfcc_data_train_mfcc1)\n",
    "    x2_train = read_mfcc_data(mfcc_data_train_mfcc2) \n",
    "    y_train = read_mfcc_data(mfcc_data_train_label)\n",
    "    \n",
    "    \n",
    "    siamese_model.fit([x1_train, x2_train], y_train, epochs=default_epochs, batch_size=default_batch_size)\n",
    "    \n",
    "    \n",
    "    x1_test = read_mfcc_data(mfcc_data_val_mfcc1)\n",
    "    x2_test = read_mfcc_data(mfcc_data_val_mfcc2)\n",
    "    y_test = read_mfcc_data(mfcc_data_val_label) \n",
    "    \n",
    "    loss, accuracy = siamese_model.evaluate([x1_test, x2_test], y_test)    \n",
    "    \n",
    "    siamese_model.save(default_model_path+\"/speech_siamese.h5\")\n",
    "\n",
    "    print(loss)\n",
    "    return accuracy\n",
    "\n",
    "def siamese_test():\n",
    "    siamese_model = keras.models.load_model(default_model_path+\"/speech_siamese.h5\")    \n",
    "    \n",
    "    x1_test = read_mfcc_data(mfcc_data_test_mfcc1) \n",
    "    x2_test = read_mfcc_data(mfcc_data_test_mfcc2) \n",
    "    y_test = read_mfcc_data(mfcc_data_test_label)\n",
    "    \n",
    "    loss, accuracy = siamese_model.test_on_batch(x=[x1_test, x2_test], y=y_test)\n",
    "    print(loss)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (50000, 32, 128)\n",
      "shape: (50000, 32, 128)\n",
      "shape: (50000,)\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 159s 3ms/step - loss: 0.6873 - acc: 0.5463\n",
      "Epoch 2/10\n",
      "20896/50000 [===========>..................] - ETA: 1:30 - loss: 0.6695 - acc: 0.5847"
     ]
    }
   ],
   "source": [
    "#wav_mfcc = load_wav_mfcc(\"/Users/hermitwang/Downloads/speech_dataset/backward/0a2b400e_nohash_0.wav\")\n",
    "#print(wav_mfcc.shape) \n",
    "score=siamese_train(local_siamese_mode='abs')\n",
    "print(score)\n",
    "score=siamese_test()\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the fact\n",
    "\n",
    "Epoch 1/10\n",
    "10000/10000 [==============================] - 108s 11ms/step - loss: 0.6679 - acc: 0.5937\n",
    "Epoch 2/10\n",
    "10000/10000 [==============================] - 99s 10ms/step - loss: 0.5817 - acc: 0.6971\n",
    "Epoch 3/10\n",
    "10000/10000 [==============================] - 99s 10ms/step - loss: 0.5324 - acc: 0.7306\n",
    "Epoch 4/10\n",
    "10000/10000 [==============================] - 96s 10ms/step - loss: 0.4799 - acc: 0.7684\n",
    "Epoch 5/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.4271 - acc: 0.8066\n",
    "Epoch 6/10\n",
    "10000/10000 [==============================] - 96s 10ms/step - loss: 0.3796 - acc: 0.8310\n",
    "Epoch 7/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.3459 - acc: 0.8511\n",
    "Epoch 8/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.3103 - acc: 0.8694\n",
    "Epoch 9/10\n",
    "10000/10000 [==============================] - 96s 10ms/step - loss: 0.2784 - acc: 0.8844\n",
    "Epoch 10/10\n",
    "10000/10000 [==============================] - 96s 10ms/step - loss: 0.2480 - acc: 0.8979\n",
    "100/100 [==============================] - 1s 11ms/step\n",
    "0.5532743597030639\n",
    "0.79\n",
    "1.0078126\n",
    "0.63\n",
    "\n",
    "GRU*256 -> Dense 1024\n",
    "Epoch 1/10\n",
    "10000/10000 [==============================] - 108s 11ms/step - loss: 0.6706 - acc: 0.5843\n",
    "Epoch 2/10\n",
    "10000/10000 [==============================] - 99s 10ms/step - loss: 0.5864 - acc: 0.6882\n",
    "Epoch 3/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.5325 - acc: 0.7314\n",
    "Epoch 4/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.4816 - acc: 0.7734\n",
    "Epoch 5/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.4413 - acc: 0.7942\n",
    "Epoch 6/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.3941 - acc: 0.8206\n",
    "Epoch 7/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.3655 - acc: 0.8393\n",
    "Epoch 8/10\n",
    "10000/10000 [==============================] - 98s 10ms/step - loss: 0.3162 - acc: 0.8612\n",
    "Epoch 9/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.2839 - acc: 0.8804\n",
    "Epoch 10/10\n",
    "10000/10000 [==============================] - 98s 10ms/step - loss: 0.2514 - acc: 0.8980\n",
    "100/100 [==============================] - 1s 13ms/step\n",
    "0.6746552109718322\n",
    "0.71\n",
    "0.7804642\n",
    "0.62\n",
    "\n",
    "\n",
    "3*GRU * 256 -> Dense 1024\n",
    "Epoch 1/10\n",
    "10000/10000 [==============================] - 380s 38ms/step - loss: 0.6932 - acc: 0.5577\n",
    "Epoch 2/10\n",
    "10000/10000 [==============================] - 364s 36ms/step - loss: 0.6512 - acc: 0.6203\n",
    "Epoch 3/10\n",
    "10000/10000 [==============================] - 363s 36ms/step - loss: 0.6079 - acc: 0.6744\n",
    "Epoch 4/10\n",
    "10000/10000 [==============================] - 363s 36ms/step - loss: 0.5538 - acc: 0.7181\n",
    "Epoch 5/10\n",
    "10000/10000 [==============================] - 365s 36ms/step - loss: 0.5094 - acc: 0.7537\n",
    "Epoch 6/10\n",
    "10000/10000 [==============================] - 364s 36ms/step - loss: 0.4635 - acc: 0.7846\n",
    "Epoch 7/10\n",
    "10000/10000 [==============================] - 362s 36ms/step - loss: 0.4153 - acc: 0.8133\n",
    "Epoch 8/10\n",
    "10000/10000 [==============================] - 366s 37ms/step - loss: 0.3677 - acc: 0.8407\n",
    "Epoch 9/10\n",
    "10000/10000 [==============================] - 363s 36ms/step - loss: 0.3252 - acc: 0.8616\n",
    "Epoch 10/10\n",
    "10000/10000 [==============================] - 366s 37ms/step - loss: 0.2846 - acc: 0.8786\n",
    "100/100 [==============================] - 1s 14ms/step\n",
    "0.6436352944374084\n",
    "0.72\n",
    "0.812591\n",
    "0.67\n",
    "\n",
    "GRU*256-> Dense 256\n",
    "Epoch 1/10\n",
    "10000/10000 [==============================] - 103s 10ms/step - loss: 0.6787 - acc: 0.5622\n",
    "Epoch 2/10\n",
    "10000/10000 [==============================] - 95s 10ms/step - loss: 0.6169 - acc: 0.6618\n",
    "Epoch 3/10\n",
    "10000/10000 [==============================] - 96s 10ms/step - loss: 0.5689 - acc: 0.7099\n",
    "Epoch 4/10\n",
    "10000/10000 [==============================] - 94s 9ms/step - loss: 0.5268 - acc: 0.7364\n",
    "Epoch 5/10\n",
    "10000/10000 [==============================] - 93s 9ms/step - loss: 0.4821 - acc: 0.7714\n",
    "Epoch 6/10\n",
    "10000/10000 [==============================] - 94s 9ms/step - loss: 0.4511 - acc: 0.7908\n",
    "Epoch 7/10\n",
    "10000/10000 [==============================] - 94s 9ms/step - loss: 0.4146 - acc: 0.8090\n",
    "Epoch 8/10\n",
    "10000/10000 [==============================] - 93s 9ms/step - loss: 0.3784 - acc: 0.8345\n",
    "Epoch 9/10\n",
    "10000/10000 [==============================] - 94s 9ms/step - loss: 0.3437 - acc: 0.8474\n",
    "Epoch 10/10\n",
    "10000/10000 [==============================] - 94s 9ms/step - loss: 0.3018 - acc: 0.8764\n",
    "100/100 [==============================] - 2s 22ms/step\n",
    "0.4378182792663574\n",
    "0.76\n",
    "0.58927524\n",
    "0.73\n",
    "\n",
    "Epoch 1/10\n",
    "10000/10000 [==============================] - 107s 11ms/step - loss: 0.6708 - acc: 0.5770\n",
    "Epoch 2/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.6115 - acc: 0.6622\n",
    "Epoch 3/10\n",
    "10000/10000 [==============================] - 95s 10ms/step - loss: 0.5634 - acc: 0.7096\n",
    "Epoch 4/10\n",
    "10000/10000 [==============================] - 95s 9ms/step - loss: 0.5310 - acc: 0.7351\n",
    "Epoch 5/10\n",
    "10000/10000 [==============================] - 95s 10ms/step - loss: 0.4971 - acc: 0.7621\n",
    "Epoch 6/10\n",
    "10000/10000 [==============================] - 99s 10ms/step - loss: 0.4523 - acc: 0.7915\n",
    "Epoch 7/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.4247 - acc: 0.8069\n",
    "Epoch 8/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.3903 - acc: 0.8284\n",
    "Epoch 9/10\n",
    "10000/10000 [==============================] - 97s 10ms/step - loss: 0.3569 - acc: 0.8445\n",
    "Epoch 10/10\n",
    "10000/10000 [==============================] - 98s 10ms/step - loss: 0.3255 - acc: 0.8616\n",
    "100/100 [==============================] - 3s 26ms/step\n",
    "0.5993771600723267\n",
    "0.71\n",
    "0.8117188\n",
    "0.67\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
